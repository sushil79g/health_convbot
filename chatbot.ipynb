{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sushil79g/health_convbot/blob/master/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lp8zl2ZVXpg",
        "colab_type": "code",
        "outputId": "1da2477f-31f0-4eab-9aa5-8b4d3392a899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdMTyrJQVKgO",
        "colab_type": "code",
        "outputId": "22fcb8fd-5aa6-4991-841d-a682e5219a39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "base_folder = 'drive/My Drive/dataset/'\n",
        "!pip install contractions\n",
        "!pip install textsearch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting contractions\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/2a/ba0a3812e2a1de2cc4ee0ded0bdb750a7cef1631c13c78a4fc4ab042adec/contractions-0.0.21-py2.py3-none-any.whl\n",
            "Installing collected packages: contractions\n",
            "Successfully installed contractions-0.0.21\n",
            "Collecting textsearch\n",
            "  Downloading https://files.pythonhosted.org/packages/42/a8/03407021f9555043de5492a2bd7a35c56cc03c2510092b5ec018cae1bbf1/textsearch-0.0.17-py2.py3-none-any.whl\n",
            "Collecting pyahocorasick (from textsearch)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/9f/f0d8e8850e12829eea2e778f1c90e3c53a9a799b7f412082a5d21cd19ae1/pyahocorasick-1.4.0.tar.gz (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 5.0MB/s \n",
            "\u001b[?25hCollecting Unidecode (from textsearch)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 49.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/90/61/87a55f5b459792fbb2b7ba6b31721b06ff5cf6bde541b40994\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, Unidecode, textsearch\n",
            "Successfully installed Unidecode-1.1.1 pyahocorasick-1.4.0 textsearch-0.0.17\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yxpx4dFFVI_7",
        "colab_type": "code",
        "outputId": "0722faeb-5c58-4c5e-9a9b-8fbcd876163f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Spyder Editor\n",
        "\n",
        "This is a temporary script file.\n",
        "\"\"\"\n",
        "import re\n",
        "import pandas as pd\n",
        "import contractions\n",
        "\n",
        "contractions.add(\"c'mon\", 'come on')\n",
        "\n",
        "file = pd.read_csv(base_folder + 'friends_chat.txt',sep='\\t')\n",
        "lines = file['line']\n",
        "\n",
        "print('Initial chats')\n",
        "for i in range(10):\n",
        "    print(lines[i])\n",
        "    \n",
        "def cleanchat(line):\n",
        "    #to convert text to lower case\n",
        "    line = line.lower()\n",
        "    #to remove ending EOL\n",
        "    line = re.sub(r'\\n','',line) \n",
        "    #re-format punctuations\n",
        "    line = re.sub(r\"[-()]\", \"\", line)\n",
        "    line = re.sub(r\"\\.\", \" .\", line)\n",
        "    line = re.sub(r\"\\!\", \" !\", line)\n",
        "    line = re.sub(r\"\\?\", \" ?\", line)\n",
        "    line = re.sub(r\"\\,\", \" ,\", line)\n",
        "    \n",
        "    #string replacement\n",
        "    line = re.sub(r\"i'm\", \"i am\", line)\n",
        "    line = re.sub(r\"he's\", \"he is\", line)\n",
        "    line = re.sub(r\"she's\", \"she is\", line)\n",
        "    line = re.sub(r\"it's\", \"it is\", line)\n",
        "    line = re.sub(r\"that's\", \"that is\", line)\n",
        "    line = re.sub(r\"what's\", \"that is\", line)\n",
        "    line = re.sub(r\"\\'ll\", \" will\", line)\n",
        "    line = re.sub(r\"\\'re\", \" are\", line)\n",
        "    line = re.sub(r\"won't\", \"will not\", line)\n",
        "    line = re.sub(r\"can't\", \"cannot\", line)\n",
        "    line = re.sub(r\"n't\", \" not\", line)\n",
        "    line = re.sub(r\"n'\", \"ng\", line)\n",
        "    line = re.sub(r\"ohh\", \"oh\", line)\n",
        "    line = re.sub(r\"ohhh\", \"oh\", line)\n",
        "    line = re.sub(r\"ohhhh\", \"oh\", line)\n",
        "    line = re.sub(r\"ohhhhh\", \"oh\", line)\n",
        "    line = re.sub(r\"ohhhhhh\", \"oh\", line)\n",
        "    line = re.sub(r\"ahh\", \"ah\", line)\n",
        "    \n",
        "    return line\n",
        "    "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial chats\n",
            "There's nothing to tell! He's just some guy I work with!\n",
            "C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
            "Alright Joey, be nice. So does he have a hump? A hump and a hairpiece?\n",
            "Wait, does he eat chalk?\n",
            "Just, 'cause, I don't want her to go through what I went through with Carl- oh!\n",
            "Okay, everybody relax. This is not even a date. It's just two people going out to dinner and not having sex.\n",
            "Sounds like a date to me.\n",
            "Alright, so I'm back in high school, I'm standing in the middle of the cafeteria, and I realize I am totally naked.\n",
            "Oh, yeah. Had that dream.\n",
            "Then I look down, and I realize there's a phone... there.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOyc5cEHVKed",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbXIT1rpVJAF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(x):\n",
        "    clean = cleanchat(x)\n",
        "    processed = contractions.fix(clean)\n",
        "    return processed\n",
        "\n",
        "file['processed_text'] = file['line'].apply(preprocess)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo0PtzYBVJAK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def length(x):\n",
        "    return len(x.split())\n",
        "file['length'] = file['line'].apply(length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XylRPRXcVJAP",
        "colab_type": "code",
        "outputId": "0d0b1964-216d-47ec-fb0d-a09b7711efd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "file['length'].describe()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    60849.000000\n",
              "mean        10.161416\n",
              "std         10.438841\n",
              "min          1.000000\n",
              "25%          3.000000\n",
              "50%          7.000000\n",
              "75%         14.000000\n",
              "max        195.000000\n",
              "Name: length, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-bjqFQQVJAV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_WORD_LENGTH = 15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIkr4bR9VJAa",
        "colab_type": "code",
        "outputId": "dfec6524-8260-4dcb-cd27-f2a6c93666ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "file.head(5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>scene_id</th>\n",
              "      <th>person</th>\n",
              "      <th>gender</th>\n",
              "      <th>original_line</th>\n",
              "      <th>line</th>\n",
              "      <th>metadata</th>\n",
              "      <th>filename</th>\n",
              "      <th>processed_text</th>\n",
              "      <th>length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>MONICA</td>\n",
              "      <td>F</td>\n",
              "      <td>Monica: There's nothing to tell! He's just som...</td>\n",
              "      <td>There's nothing to tell! He's just some guy I ...</td>\n",
              "      <td>There_EX 's_VBZ nothing_PN1 to_TO tell_VVI !_!...</td>\n",
              "      <td>0101.txt</td>\n",
              "      <td>there is nothing to tell ! he is just some guy...</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>101</td>\n",
              "      <td>1</td>\n",
              "      <td>JOEY</td>\n",
              "      <td>M</td>\n",
              "      <td>Joey: C'mon, you're going out with the guy! Th...</td>\n",
              "      <td>C'mon, you're going out with the guy! There's ...</td>\n",
              "      <td>C'm_VV0 on_RP you_PPY 're_VBR going_VVG out_RP...</td>\n",
              "      <td>0101.txt</td>\n",
              "      <td>come on , you are going out with the guy ! the...</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>201</td>\n",
              "      <td>1</td>\n",
              "      <td>CHANDLER</td>\n",
              "      <td>M</td>\n",
              "      <td>Chandler: All right Joey, be nice.  So does he...</td>\n",
              "      <td>Alright Joey, be nice. So does he have a hump?...</td>\n",
              "      <td>All_RR21 right_RR22 Joey_NP1 be_VBI nice_JJ ._...</td>\n",
              "      <td>0101.txt</td>\n",
              "      <td>alright joey , be nice . so does he have a hum...</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>301</td>\n",
              "      <td>1</td>\n",
              "      <td>PHOEBE</td>\n",
              "      <td>F</td>\n",
              "      <td>Phoebe: Wait, does he eat chalk?</td>\n",
              "      <td>Wait, does he eat chalk?</td>\n",
              "      <td>Wait_VV0 does_VDZ he_PPHS1 eat_VVI chalk_NN1 ?_?</td>\n",
              "      <td>0101.txt</td>\n",
              "      <td>wait , does he eat chalk ?</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>401</td>\n",
              "      <td>1</td>\n",
              "      <td>PHOEBE</td>\n",
              "      <td>F</td>\n",
              "      <td>Phoebe: Just, 'cause, I don't want her to go t...</td>\n",
              "      <td>Just, 'cause, I don't want her to go through w...</td>\n",
              "      <td>Just_RR 'cause_CS I_PPIS1 do_VD0 n't_XX want_V...</td>\n",
              "      <td>0101.txt</td>\n",
              "      <td>just , because , i do not want her to go throu...</td>\n",
              "      <td>16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    id scene_id  ...                                     processed_text length\n",
              "0    1        1  ...  there is nothing to tell ! he is just some guy...     11\n",
              "1  101        1  ...  come on , you are going out with the guy ! the...     14\n",
              "2  201        1  ...  alright joey , be nice . so does he have a hum...     15\n",
              "3  301        1  ...                         wait , does he eat chalk ?      5\n",
              "4  401        1  ...  just , because , i do not want her to go throu...     16\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3Vl5UZxVJAh",
        "colab_type": "code",
        "outputId": "9e39fa7b-c5aa-466a-d00a-3d1a570a3b38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "file.isnull().sum()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id                0\n",
              "scene_id          0\n",
              "person            0\n",
              "gender            0\n",
              "original_line     0\n",
              "line              0\n",
              "metadata          0\n",
              "filename          0\n",
              "processed_text    0\n",
              "length            0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luxTNKJRVJAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "temp_dataframe = file[['person','processed_text']]\n",
        "remove_index = []\n",
        "chat_pair = []\n",
        "i = 0\n",
        "while i <= (len(temp_dataframe)-2):\n",
        "    raw_i = i\n",
        "    \n",
        "    while True:\n",
        "#         print(raw_i,i, i+1)\n",
        "        if temp_dataframe.iloc[i+1]['person'] == temp_dataframe.iloc[i]['person']:\n",
        "            temp_dataframe.iloc[raw_i]['processed_text'] += ','+ temp_dataframe.iloc[i+1]['processed_text']\n",
        "            chat_pair.append([temp_dataframe.iloc[raw_i]['processed_text'], temp_dataframe.iloc[raw_i+1]['processed_text']])\n",
        "            remove_index.append(i+1)\n",
        "        else:\n",
        "            break\n",
        "        i = i+1\n",
        "        \n",
        "    i += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1c0NB2GVJAs",
        "colab_type": "code",
        "outputId": "625fc055-0fd4-494c-a0a7-acba7884db90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "temp_dataframe.drop(remove_index, inplace=True)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  errors=errors)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euCMB7vWVJAz",
        "colab_type": "code",
        "outputId": "09a58cd6-1c36-4192-dff4-22f7e0e2ec05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "temp_dataframe.head(5)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>person</th>\n",
              "      <th>processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MONICA</td>\n",
              "      <td>there is nothing to tell ! he is just some guy...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>JOEY</td>\n",
              "      <td>come on , you are going out with the guy ! the...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>CHANDLER</td>\n",
              "      <td>alright joey , be nice . so does he have a hum...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>PHOEBE</td>\n",
              "      <td>wait , does he eat chalk ?,just , because , i ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>MONICA</td>\n",
              "      <td>okay , everybody relax . this is not even a da...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     person                                     processed_text\n",
              "0    MONICA  there is nothing to tell ! he is just some guy...\n",
              "1      JOEY  come on , you are going out with the guy ! the...\n",
              "2  CHANDLER  alright joey , be nice . so does he have a hum...\n",
              "3    PHOEBE  wait , does he eat chalk ?,just , because , i ...\n",
              "5    MONICA  okay , everybody relax . this is not even a da..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Jy1YspFVJA5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# contractions.add(\"c'mon\", 'come on')\n",
        "list_of_word = list(temp_dataframe['processed_text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yS05rheOVJA-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = ' '.join(list_of_word).split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_l2Rde4ZTc9R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dee9ebfb-3528-444f-ae43-50498fad8424"
      },
      "source": [
        "len(words)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "836085"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs_AsvdrVJBC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "wordcount = Counter(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zzgce2RwVJBG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab= set(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnC1lraZPnAG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "23f3b9f3-5a7a-4690-ba2d-b6ca631ed9e2"
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17828"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGS681T_VJBK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_threshold = []\n",
        "for word in vocab:\n",
        "    if wordcount[word] >=3:\n",
        "        word_threshold.append(word)\n",
        "#     break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb_fRhb-VJBO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# word2ind ={{word, index} for word,index in enumerate(list(word_threshold))}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_Iari2QVJBW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2index = {}\n",
        "index2word = {}\n",
        "for index,word in enumerate(word_threshold):\n",
        "    word2index.update({word:index})\n",
        "    index2word.update({index:word})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqyGrls5VJBd",
        "colab_type": "code",
        "outputId": "003a6bf7-55c3-4099-afd5-5a2b19d7271a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from pprint import pprint\n",
        "# pprint('total word in vocab',len(word_threshold))\n",
        "len(word_threshold)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6756"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PUIhn2pVJBk",
        "colab_type": "code",
        "outputId": "c6e295ce-8620-4fa3-bf58-44d789ca0dc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "(word2index)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'chandler': 0,\n",
              " 'pinned': 1,\n",
              " 'whoohoo': 2,\n",
              " 'honesty': 3,\n",
              " 'climb': 4,\n",
              " 'whaddya': 5,\n",
              " 'ronni': 6,\n",
              " 'ashamed': 7,\n",
              " 'newman': 8,\n",
              " 'spoiled': 9,\n",
              " 'player': 10,\n",
              " 'mature': 11,\n",
              " 'maybe': 12,\n",
              " 'accounts': 13,\n",
              " 'considering': 14,\n",
              " \"moment's\": 15,\n",
              " 'st': 16,\n",
              " 'handful': 17,\n",
              " 'looong': 18,\n",
              " 'reasons': 19,\n",
              " 'docks': 20,\n",
              " 'marjorie': 21,\n",
              " \"richard's\": 22,\n",
              " 'kenny': 23,\n",
              " 'engagement': 24,\n",
              " '!,my': 25,\n",
              " 'bought': 26,\n",
              " 'routine': 27,\n",
              " 'caitlin': 28,\n",
              " 'ordered': 29,\n",
              " 'cutie': 30,\n",
              " '36': 31,\n",
              " 'discuss': 32,\n",
              " 'omelet': 33,\n",
              " 'unplug': 34,\n",
              " 'impressive': 35,\n",
              " 'contractions': 36,\n",
              " \"'days\": 37,\n",
              " 'otherwise': 38,\n",
              " 'hunt': 39,\n",
              " 'easier': 40,\n",
              " 'thirty': 41,\n",
              " 'tastes': 42,\n",
              " 'she': 43,\n",
              " 'underwear': 44,\n",
              " 'pushing': 45,\n",
              " 'checkout': 46,\n",
              " 'mouth': 47,\n",
              " 'scrud': 48,\n",
              " 'vows': 49,\n",
              " 'cerebral': 50,\n",
              " 'roommates': 51,\n",
              " 'propose': 52,\n",
              " 'saliva': 53,\n",
              " 'harder': 54,\n",
              " 'stronger': 55,\n",
              " 'presenting': 56,\n",
              " 'butts': 57,\n",
              " 'celtics': 58,\n",
              " 'ukrainian': 59,\n",
              " 'scratchy': 60,\n",
              " 'sure': 61,\n",
              " 'carry': 62,\n",
              " 'barbara': 63,\n",
              " 'cushion': 64,\n",
              " 'peas': 65,\n",
              " 'thanked': 66,\n",
              " 'tournament': 67,\n",
              " 'going': 68,\n",
              " 'shoulda': 69,\n",
              " 'sick': 70,\n",
              " 'companies': 71,\n",
              " 'swelling': 72,\n",
              " 'callback': 73,\n",
              " 'dishes': 74,\n",
              " \"alessandro's\": 75,\n",
              " 'bigbig': 76,\n",
              " '4:30': 77,\n",
              " 'musta': 78,\n",
              " 'stairs': 79,\n",
              " 'ham': 80,\n",
              " 'sake': 81,\n",
              " '29': 82,\n",
              " 'response': 83,\n",
              " 'jesus': 84,\n",
              " '911': 85,\n",
              " 'taco': 86,\n",
              " 'king': 87,\n",
              " 'hanukkah': 88,\n",
              " 'gifts': 89,\n",
              " 'series': 90,\n",
              " 'lobster': 91,\n",
              " 'hardware': 92,\n",
              " 'nowhere': 93,\n",
              " 'erin': 94,\n",
              " 'general': 95,\n",
              " 'aand': 96,\n",
              " 'medical': 97,\n",
              " 'de': 98,\n",
              " 'mariongs': 99,\n",
              " 'pbs': 100,\n",
              " 'useless': 101,\n",
              " 'alcohol': 102,\n",
              " 'dwha': 103,\n",
              " 'safe': 104,\n",
              " 'bong': 105,\n",
              " 'dropping': 106,\n",
              " '5:30': 107,\n",
              " 'cheated': 108,\n",
              " 'eyes': 109,\n",
              " 'shoulder': 110,\n",
              " 'lilies': 111,\n",
              " 'cooper': 112,\n",
              " 'ironic': 113,\n",
              " 'born': 114,\n",
              " 'shy': 115,\n",
              " 'keeps': 116,\n",
              " 'borrowed': 117,\n",
              " '?': 118,\n",
              " 'moving': 119,\n",
              " 'heston': 120,\n",
              " 'centerpieces': 121,\n",
              " 'client': 122,\n",
              " 'olivia': 123,\n",
              " 'spill': 124,\n",
              " 'authentic': 125,\n",
              " 'pictionary': 126,\n",
              " 'says': 127,\n",
              " 'officer': 128,\n",
              " 'resumes': 129,\n",
              " 'waffle': 130,\n",
              " 'drives': 131,\n",
              " 'neighbors': 132,\n",
              " 'awkward': 133,\n",
              " 'professor': 134,\n",
              " 'directing': 135,\n",
              " 'writer': 136,\n",
              " 'subway': 137,\n",
              " 'cooking': 138,\n",
              " 'autograph': 139,\n",
              " 'cut': 140,\n",
              " 'hillary': 141,\n",
              " 'wondering': 142,\n",
              " 'choked': 143,\n",
              " 'weee': 144,\n",
              " 'whattaya': 145,\n",
              " 'era': 146,\n",
              " 'clogs': 147,\n",
              " 'used': 148,\n",
              " 'custard': 149,\n",
              " '.,wait': 150,\n",
              " 'computer': 151,\n",
              " 'fever': 152,\n",
              " 'ken': 153,\n",
              " 'chuck': 154,\n",
              " '.,see': 155,\n",
              " 'undo': 156,\n",
              " '$12': 157,\n",
              " 'statement': 158,\n",
              " 'weak': 159,\n",
              " '$3': 160,\n",
              " 'funnier': 161,\n",
              " 'tradition': 162,\n",
              " \"'chand\": 163,\n",
              " 'gain': 164,\n",
              " '48': 165,\n",
              " 'clunkers': 166,\n",
              " \"'no\": 167,\n",
              " 'appointment': 168,\n",
              " '.and': 169,\n",
              " 'swoop': 170,\n",
              " 'boss': 171,\n",
              " 'kisser': 172,\n",
              " 'terrace': 173,\n",
              " 'great': 174,\n",
              " 'sooo': 175,\n",
              " 'barn': 176,\n",
              " \"everything's\": 177,\n",
              " 'matter': 178,\n",
              " 'rossie': 179,\n",
              " 'marijuana': 180,\n",
              " 'flirt': 181,\n",
              " 'screwing': 182,\n",
              " 'wrapped': 183,\n",
              " 'banging': 184,\n",
              " 'showering': 185,\n",
              " 'wedgie': 186,\n",
              " 'suite': 187,\n",
              " 'soso': 188,\n",
              " 'madam': 189,\n",
              " 'humor': 190,\n",
              " 'failed': 191,\n",
              " 'mmm': 192,\n",
              " 'lawyer': 193,\n",
              " 'pill': 194,\n",
              " 'me;': 195,\n",
              " 'admissions': 196,\n",
              " 'areyou': 197,\n",
              " 'is': 198,\n",
              " 'schedule': 199,\n",
              " 'altar': 200,\n",
              " 'bowl': 201,\n",
              " 'responsibilities': 202,\n",
              " 'regina': 203,\n",
              " 'costumes': 204,\n",
              " 'comfortable': 205,\n",
              " 'brains': 206,\n",
              " 'complex': 207,\n",
              " 'granddaughter': 208,\n",
              " \"today's\": 209,\n",
              " \"'ya\": 210,\n",
              " '.is': 211,\n",
              " 'begins': 212,\n",
              " 'yeah': 213,\n",
              " 'facing': 214,\n",
              " 'begley': 215,\n",
              " 'wth': 216,\n",
              " 'original': 217,\n",
              " 'described': 218,\n",
              " 'maker': 219,\n",
              " 'max': 220,\n",
              " 'no': 221,\n",
              " 'heyheyhey': 222,\n",
              " 'boobs': 223,\n",
              " 'nora': 224,\n",
              " 'ummm': 225,\n",
              " 'tiara': 226,\n",
              " 'despite': 227,\n",
              " 'sapiens': 228,\n",
              " 'marrying': 229,\n",
              " 'clark': 230,\n",
              " 'truck': 231,\n",
              " 'bills': 232,\n",
              " 'gauze': 233,\n",
              " 'poison': 234,\n",
              " 'muscles': 235,\n",
              " '.g': 236,\n",
              " \"d'you\": 237,\n",
              " 'debbie': 238,\n",
              " 'brings': 239,\n",
              " 'sabbatical': 240,\n",
              " 'ahh': 241,\n",
              " 'hugging': 242,\n",
              " 'becker': 243,\n",
              " 'timing': 244,\n",
              " 'allall': 245,\n",
              " 'boxer': 246,\n",
              " 'kissing': 247,\n",
              " 'wicker': 248,\n",
              " 'political': 249,\n",
              " 'geology': 250,\n",
              " 'apologized': 251,\n",
              " 'thquirt': 252,\n",
              " 'philange': 253,\n",
              " 'drum': 254,\n",
              " \"grandmother's\": 255,\n",
              " 'super': 256,\n",
              " 'flush': 257,\n",
              " '8': 258,\n",
              " 'trail': 259,\n",
              " 'handcuffs': 260,\n",
              " 'picture': 261,\n",
              " 'chimney': 262,\n",
              " 'heh': 263,\n",
              " 'dove': 264,\n",
              " 'gum': 265,\n",
              " 'goals': 266,\n",
              " 'wife': 267,\n",
              " 'letting': 268,\n",
              " 'basketball': 269,\n",
              " 'thosethose': 270,\n",
              " 'avenue': 271,\n",
              " 'waiters': 272,\n",
              " 'dip': 273,\n",
              " 'lent': 274,\n",
              " 'raymond': 275,\n",
              " \"g'night\": 276,\n",
              " 'finest': 277,\n",
              " 'crack': 278,\n",
              " 'fossil': 279,\n",
              " 'paste': 280,\n",
              " 'stayed': 281,\n",
              " 'screening': 282,\n",
              " 'chump': 283,\n",
              " \"world's\": 284,\n",
              " 'science': 285,\n",
              " 'performance': 286,\n",
              " 'early': 287,\n",
              " 'infomercial': 288,\n",
              " 'oho': 289,\n",
              " 'dining': 290,\n",
              " 'forbid': 291,\n",
              " 'ritual': 292,\n",
              " 'burst': 293,\n",
              " 'kyle': 294,\n",
              " 'darn': 295,\n",
              " 'catering': 296,\n",
              " 'deeds': 297,\n",
              " 'mean': 298,\n",
              " 'laying': 299,\n",
              " 'pound': 300,\n",
              " 'borrow': 301,\n",
              " 'mints': 302,\n",
              " 'application': 303,\n",
              " 'egg': 304,\n",
              " 'doin': 305,\n",
              " 'burns': 306,\n",
              " 'ruined': 307,\n",
              " '.,now': 308,\n",
              " 'mud': 309,\n",
              " \"what'd\": 310,\n",
              " 'arrogant': 311,\n",
              " 'physically': 312,\n",
              " 'difference': 313,\n",
              " 'lives': 314,\n",
              " 'girlfriends': 315,\n",
              " 'drove': 316,\n",
              " 'nick': 317,\n",
              " 'customer': 318,\n",
              " 'atlanta': 319,\n",
              " 'phd': 320,\n",
              " 'braces': 321,\n",
              " 'did': 322,\n",
              " 'victim': 323,\n",
              " 'worse': 324,\n",
              " 'guys': 325,\n",
              " 'form': 326,\n",
              " 'gold': 327,\n",
              " 'option': 328,\n",
              " 'names': 329,\n",
              " 'traffic': 330,\n",
              " 'achieve': 331,\n",
              " 'diary': 332,\n",
              " 'flipped': 333,\n",
              " 'wrecking': 334,\n",
              " 'bamboozled': 335,\n",
              " 'hottie': 336,\n",
              " 'drop': 337,\n",
              " 'allergies': 338,\n",
              " 'younger': 339,\n",
              " 'flingingflanging': 340,\n",
              " 'downtown': 341,\n",
              " 'dry': 342,\n",
              " 'volunteers': 343,\n",
              " 'yo': 344,\n",
              " 'rhythm': 345,\n",
              " 'city': 346,\n",
              " 'vegas': 347,\n",
              " 'ceiling': 348,\n",
              " 'coffees': 349,\n",
              " \"to'\": 350,\n",
              " 'boom': 351,\n",
              " 'expert': 352,\n",
              " 'sunny': 353,\n",
              " 'umhmm': 354,\n",
              " 'halftime': 355,\n",
              " 'shown': 356,\n",
              " 'howard': 357,\n",
              " 'vince': 358,\n",
              " 'spray': 359,\n",
              " 'does': 360,\n",
              " 'miss': 361,\n",
              " 'wellwell': 362,\n",
              " 'staten': 363,\n",
              " 'jen': 364,\n",
              " 'leak': 365,\n",
              " 'taught': 366,\n",
              " 'cuter': 367,\n",
              " 'wishbone': 368,\n",
              " '.oh': 369,\n",
              " '.will': 370,\n",
              " 'hahaha': 371,\n",
              " 'starring': 372,\n",
              " 'breaks': 373,\n",
              " 'pub': 374,\n",
              " 'punk': 375,\n",
              " 'yams': 376,\n",
              " '.in': 377,\n",
              " 'issues': 378,\n",
              " 'angelica': 379,\n",
              " 'nauh': 380,\n",
              " 'pool': 381,\n",
              " 'mark': 382,\n",
              " '13': 383,\n",
              " 'more': 384,\n",
              " 'brother': 385,\n",
              " 'concert': 386,\n",
              " 'appears': 387,\n",
              " 'bill': 388,\n",
              " 'forms': 389,\n",
              " \"ross'\": 390,\n",
              " 'explode': 391,\n",
              " 'refers': 392,\n",
              " 'cheerleader': 393,\n",
              " 'gay': 394,\n",
              " 'chinese': 395,\n",
              " 'recognize': 396,\n",
              " 'cent': 397,\n",
              " 'dressed': 398,\n",
              " 'seems': 399,\n",
              " 'cracker': 400,\n",
              " 'own': 401,\n",
              " 'rented': 402,\n",
              " 'until': 403,\n",
              " 'bid': 404,\n",
              " 'chichi': 405,\n",
              " 'doing': 406,\n",
              " 'who': 407,\n",
              " 'chocolates': 408,\n",
              " 'gogo': 409,\n",
              " \"guys'\": 410,\n",
              " 'horribly': 411,\n",
              " 'putting': 412,\n",
              " 'fling': 413,\n",
              " 'scenes': 414,\n",
              " 'goalie': 415,\n",
              " 'vestibule': 416,\n",
              " 'freezing': 417,\n",
              " 'fought': 418,\n",
              " 'hello': 419,\n",
              " 'moms': 420,\n",
              " 'crush': 421,\n",
              " 'pickle': 422,\n",
              " 'primary': 423,\n",
              " 'brian': 424,\n",
              " 'professional': 425,\n",
              " 'steve': 426,\n",
              " 'ball': 427,\n",
              " 'attention': 428,\n",
              " 'hammock': 429,\n",
              " 'hexadrin': 430,\n",
              " 'forgot': 431,\n",
              " 'round': 432,\n",
              " \".y'know\": 433,\n",
              " 'helping': 434,\n",
              " 'strongest': 435,\n",
              " 'common': 436,\n",
              " 'sudden': 437,\n",
              " \"joshua's\": 438,\n",
              " 'wisdom': 439,\n",
              " 'cream': 440,\n",
              " 'venture': 441,\n",
              " 'gather': 442,\n",
              " 'blueberry': 443,\n",
              " 'scariest': 444,\n",
              " 'manage': 445,\n",
              " 'casual': 446,\n",
              " 'stalker': 447,\n",
              " 'shrill': 448,\n",
              " 'waswas': 449,\n",
              " 'dying': 450,\n",
              " 'weird': 451,\n",
              " 'intense': 452,\n",
              " 'jeans': 453,\n",
              " 'shellfish': 454,\n",
              " \"pete's\": 455,\n",
              " 'groom': 456,\n",
              " 'missed': 457,\n",
              " 'sticking': 458,\n",
              " 'magazines': 459,\n",
              " '15': 460,\n",
              " 'trade': 461,\n",
              " 'cheating': 462,\n",
              " \"friend's\": 463,\n",
              " 'burke': 464,\n",
              " 'deed': 465,\n",
              " 'passions': 466,\n",
              " 'duties': 467,\n",
              " 'liar': 468,\n",
              " 'charity': 469,\n",
              " 'warned': 470,\n",
              " 'blanket': 471,\n",
              " 'picnic': 472,\n",
              " 'salad': 473,\n",
              " 'competitive': 474,\n",
              " 'lemonade': 475,\n",
              " 'hilda': 476,\n",
              " 'sixes': 477,\n",
              " 'cuffs': 478,\n",
              " 'nicely': 479,\n",
              " \"tomorrow's\": 480,\n",
              " 'chippy': 481,\n",
              " 'ducks': 482,\n",
              " 'about': 483,\n",
              " 'hate': 484,\n",
              " 'yelled': 485,\n",
              " 'checked': 486,\n",
              " 'butler': 487,\n",
              " 'edge': 488,\n",
              " 'recently': 489,\n",
              " 'thousands': 490,\n",
              " 'handles': 491,\n",
              " 'steep': 492,\n",
              " 'immediately': 493,\n",
              " 'casserole': 494,\n",
              " 'tears': 495,\n",
              " 'drugs': 496,\n",
              " 'cave': 497,\n",
              " 'cheek': 498,\n",
              " 'alby': 499,\n",
              " 'wears': 500,\n",
              " 'linda': 501,\n",
              " 'dumping': 502,\n",
              " 'shocking': 503,\n",
              " 'gladiator': 504,\n",
              " 'swans': 505,\n",
              " 'op': 506,\n",
              " 'spilled': 507,\n",
              " 'where': 508,\n",
              " 'mattress': 509,\n",
              " '!,there': 510,\n",
              " 'tan': 511,\n",
              " 'needing': 512,\n",
              " 'crawled': 513,\n",
              " 'bone': 514,\n",
              " 'shiny': 515,\n",
              " 'dragging': 516,\n",
              " 'morning': 517,\n",
              " 'brown': 518,\n",
              " 'able': 519,\n",
              " 'boarding': 520,\n",
              " 'motto': 521,\n",
              " 'inside': 522,\n",
              " 'erica': 523,\n",
              " 'franzblau': 524,\n",
              " 'volcano': 525,\n",
              " 'bound': 526,\n",
              " 'territory': 527,\n",
              " 'private': 528,\n",
              " 'skin': 529,\n",
              " 'paper': 530,\n",
              " 'simon': 531,\n",
              " 'mind': 532,\n",
              " 'may': 533,\n",
              " '98': 534,\n",
              " 'pies': 535,\n",
              " 'monologue': 536,\n",
              " 'cleared': 537,\n",
              " 'slow': 538,\n",
              " 'ralph': 539,\n",
              " 'atat': 540,\n",
              " 'flirting': 541,\n",
              " 'buyer': 542,\n",
              " 'taken': 543,\n",
              " 'lied': 544,\n",
              " 'trifle': 545,\n",
              " 'classy': 546,\n",
              " 'convinced': 547,\n",
              " 'pacino': 548,\n",
              " 'secretly': 549,\n",
              " 'fog': 550,\n",
              " 'blocking': 551,\n",
              " 'intimate': 552,\n",
              " 'wasting': 553,\n",
              " 'low': 554,\n",
              " 'wait': 555,\n",
              " 'bunch': 556,\n",
              " 'superman': 557,\n",
              " 'beth': 558,\n",
              " '2%': 559,\n",
              " 'sid': 560,\n",
              " 'rack': 561,\n",
              " 'deserves': 562,\n",
              " 'peel': 563,\n",
              " 'strongly': 564,\n",
              " 'system': 565,\n",
              " 'counter': 566,\n",
              " 'discussed': 567,\n",
              " \"buffay's\": 568,\n",
              " '$20': 569,\n",
              " \"'s\": 570,\n",
              " '.,my': 571,\n",
              " 'large': 572,\n",
              " 'marcel': 573,\n",
              " 'food': 574,\n",
              " 'insanely': 575,\n",
              " 'chance': 576,\n",
              " 'beat': 577,\n",
              " 'loved': 578,\n",
              " 'dearly': 579,\n",
              " 'page': 580,\n",
              " \"'no'\": 581,\n",
              " 'getting': 582,\n",
              " '10th': 583,\n",
              " '?,ross': 584,\n",
              " 'advice': 585,\n",
              " \"y'know\": 586,\n",
              " 'legal': 587,\n",
              " 'straighten': 588,\n",
              " 'climbing': 589,\n",
              " 'course': 590,\n",
              " 'oo': 591,\n",
              " 'wesley': 592,\n",
              " 'handed': 593,\n",
              " 'fossils': 594,\n",
              " 'circle': 595,\n",
              " '!,hello': 596,\n",
              " \"geller's\": 597,\n",
              " 'taste': 598,\n",
              " 'moved': 599,\n",
              " 'pal': 600,\n",
              " 'fogged': 601,\n",
              " 'lovers': 602,\n",
              " 'told': 603,\n",
              " 'dilated': 604,\n",
              " 'singing': 605,\n",
              " 'barbecue': 606,\n",
              " 'spite': 607,\n",
              " 'costume': 608,\n",
              " 'dinosaurs': 609,\n",
              " 'characters': 610,\n",
              " '15th': 611,\n",
              " 'dialing': 612,\n",
              " 'foundation': 613,\n",
              " 'everyone': 614,\n",
              " 'cleansing': 615,\n",
              " 'offered': 616,\n",
              " 'rehearsal': 617,\n",
              " 'punches': 618,\n",
              " 'scrappy': 619,\n",
              " 'complicated': 620,\n",
              " 'chocolate': 621,\n",
              " 'bag': 622,\n",
              " 'lead': 623,\n",
              " 'moore': 624,\n",
              " 'concern': 625,\n",
              " 'complement': 626,\n",
              " 'harm': 627,\n",
              " 'popped': 628,\n",
              " 'deliver': 629,\n",
              " 'fold': 630,\n",
              " 'drive': 631,\n",
              " 'dirty': 632,\n",
              " 'dad': 633,\n",
              " 'forgotten': 634,\n",
              " 'wild': 635,\n",
              " 'china': 636,\n",
              " 'children': 637,\n",
              " 'scares': 638,\n",
              " 'scalp': 639,\n",
              " 'pesto': 640,\n",
              " 'defend': 641,\n",
              " \"'do\": 642,\n",
              " 'answer': 643,\n",
              " '!,come': 644,\n",
              " '!i': 645,\n",
              " \"monkey's\": 646,\n",
              " 'operation': 647,\n",
              " 'breath': 648,\n",
              " 'took': 649,\n",
              " 'devil': 650,\n",
              " 'sickness': 651,\n",
              " 'freaked': 652,\n",
              " 'mexico': 653,\n",
              " 'working': 654,\n",
              " 'russia': 655,\n",
              " 'filling': 656,\n",
              " 'cabin': 657,\n",
              " 'bicep': 658,\n",
              " 'asses': 659,\n",
              " 'printed': 660,\n",
              " 'definitely': 661,\n",
              " 'mocking': 662,\n",
              " 'seesaw': 663,\n",
              " 'lie': 664,\n",
              " 'stoned': 665,\n",
              " 'greens': 666,\n",
              " 'wish': 667,\n",
              " 'macadamia': 668,\n",
              " 'month': 669,\n",
              " 'luckily': 670,\n",
              " 'has': 671,\n",
              " 'dream': 672,\n",
              " 'hairy': 673,\n",
              " 'prince': 674,\n",
              " 'mugging': 675,\n",
              " 'good': 676,\n",
              " 'o': 677,\n",
              " 'fed': 678,\n",
              " 'goddess': 679,\n",
              " 'boobies': 680,\n",
              " 'squeeze': 681,\n",
              " 'pit': 682,\n",
              " 'thisthis': 683,\n",
              " 'apologise': 684,\n",
              " 'eligible': 685,\n",
              " 'puff': 686,\n",
              " 'practice': 687,\n",
              " 'eight': 688,\n",
              " 'destiny': 689,\n",
              " 'gavin': 690,\n",
              " 'crazy': 691,\n",
              " 'lamaze': 692,\n",
              " 'counting': 693,\n",
              " 'committee': 694,\n",
              " 'cases': 695,\n",
              " 'tonight': 696,\n",
              " 'aloof': 697,\n",
              " 'included': 698,\n",
              " 'sang': 699,\n",
              " 'purchasing': 700,\n",
              " 'bless': 701,\n",
              " 'sister': 702,\n",
              " 'nod': 703,\n",
              " 'cujo': 704,\n",
              " 'velociraptor': 705,\n",
              " 'andie': 706,\n",
              " 'brain': 707,\n",
              " 'freak': 708,\n",
              " 'years': 709,\n",
              " 'developed': 710,\n",
              " 'actor': 711,\n",
              " 'dental': 712,\n",
              " '35th': 713,\n",
              " 'london': 714,\n",
              " 'glue': 715,\n",
              " 'goodnight': 716,\n",
              " 'impossible': 717,\n",
              " 'whack': 718,\n",
              " 'nurses': 719,\n",
              " 'step': 720,\n",
              " 'hear': 721,\n",
              " 'sloppy': 722,\n",
              " 'rockefeller': 723,\n",
              " 'started': 724,\n",
              " 'yourselves': 725,\n",
              " 'honored': 726,\n",
              " \"one's\": 727,\n",
              " 'razor': 728,\n",
              " 'higher': 729,\n",
              " 'knit': 730,\n",
              " 'teaches': 731,\n",
              " 'milkshake': 732,\n",
              " 'marc': 733,\n",
              " 'lilly': 734,\n",
              " 'sixteen': 735,\n",
              " 'devastated': 736,\n",
              " 'se': 737,\n",
              " 'scenario': 738,\n",
              " 'tree': 739,\n",
              " 'usage': 740,\n",
              " 'items': 741,\n",
              " 'paris': 742,\n",
              " 'pals': 743,\n",
              " 'willing': 744,\n",
              " 'season': 745,\n",
              " 'specifics': 746,\n",
              " 'erectus': 747,\n",
              " 'within': 748,\n",
              " 'shaft': 749,\n",
              " \"dad's\": 750,\n",
              " 'closing': 751,\n",
              " 'masculine': 752,\n",
              " 'seemed': 753,\n",
              " 'oror': 754,\n",
              " 'restaurant': 755,\n",
              " 'playstation': 756,\n",
              " 'pause': 757,\n",
              " 'both': 758,\n",
              " 'speaks': 759,\n",
              " 'mayhem': 760,\n",
              " 'adding': 761,\n",
              " 'feeling': 762,\n",
              " 'commercials': 763,\n",
              " 'boundaries': 764,\n",
              " 'fireball': 765,\n",
              " 'transplant': 766,\n",
              " 'flan': 767,\n",
              " 'questions': 768,\n",
              " 'therapy': 769,\n",
              " 'von': 770,\n",
              " 'pets': 771,\n",
              " 'dot': 772,\n",
              " 'possibility': 773,\n",
              " 'scientist': 774,\n",
              " 'field': 775,\n",
              " '18th': 776,\n",
              " 'statue': 777,\n",
              " 'oboe': 778,\n",
              " 'skirt': 779,\n",
              " 'espresso': 780,\n",
              " 'anonymous': 781,\n",
              " 'nubbin': 782,\n",
              " 'rent': 783,\n",
              " 'run': 784,\n",
              " 'adds': 785,\n",
              " 'unfortunate': 786,\n",
              " 'show': 787,\n",
              " 'aww': 788,\n",
              " 'underneath': 789,\n",
              " 'charging': 790,\n",
              " 'every': 791,\n",
              " 'lotion': 792,\n",
              " 'guts': 793,\n",
              " 'short': 794,\n",
              " 'him': 795,\n",
              " 'stress': 796,\n",
              " 'mhm': 797,\n",
              " 'market': 798,\n",
              " 'oughta': 799,\n",
              " 'selfless': 800,\n",
              " 'backfired': 801,\n",
              " 'gal': 802,\n",
              " 'bringing': 803,\n",
              " 'guide': 804,\n",
              " 'rossy': 805,\n",
              " 'kit': 806,\n",
              " 'heroin': 807,\n",
              " 'lying': 808,\n",
              " 'grows': 809,\n",
              " 'duck': 810,\n",
              " 'museum': 811,\n",
              " 'tilt': 812,\n",
              " 'spitter': 813,\n",
              " 'uses': 814,\n",
              " 'capable': 815,\n",
              " 'diaper': 816,\n",
              " 'you': 817,\n",
              " 'karatay': 818,\n",
              " 'greengs': 819,\n",
              " \"parent's\": 820,\n",
              " 'direction': 821,\n",
              " 'erm': 822,\n",
              " 'alcoholic': 823,\n",
              " 'tour': 824,\n",
              " 'lo': 825,\n",
              " 'lab': 826,\n",
              " 'finish': 827,\n",
              " 'nicer': 828,\n",
              " 'lamps': 829,\n",
              " 'bubbles': 830,\n",
              " 'drawing': 831,\n",
              " 'loveable': 832,\n",
              " 'society': 833,\n",
              " 'thursday': 834,\n",
              " 'uuuh': 835,\n",
              " 'portuguese': 836,\n",
              " 'jo': 837,\n",
              " 'exam': 838,\n",
              " 'screwed': 839,\n",
              " 'ridiculous': 840,\n",
              " 'solved': 841,\n",
              " 'patch': 842,\n",
              " 'tray': 843,\n",
              " 'twenty': 844,\n",
              " 'avoid': 845,\n",
              " 'spit': 846,\n",
              " 'lisa': 847,\n",
              " 'dads': 848,\n",
              " 'landlord': 849,\n",
              " 'dutch': 850,\n",
              " 'chances': 851,\n",
              " 'performing': 852,\n",
              " 'stew': 853,\n",
              " 'monroe': 854,\n",
              " 'baking': 855,\n",
              " 'stone': 856,\n",
              " 'himself': 857,\n",
              " 'internet': 858,\n",
              " 'warn': 859,\n",
              " 'salon': 860,\n",
              " 'pouring': 861,\n",
              " 'spank': 862,\n",
              " 'convention': 863,\n",
              " 'sheet': 864,\n",
              " 'lincoln': 865,\n",
              " 'cab': 866,\n",
              " 'huddle': 867,\n",
              " 'theoretically': 868,\n",
              " 'umm': 869,\n",
              " 'symbol': 870,\n",
              " 'carbon': 871,\n",
              " \"joanna's\": 872,\n",
              " '7': 873,\n",
              " 'sized': 874,\n",
              " 'carcass': 875,\n",
              " 'onto': 876,\n",
              " 'bitches': 877,\n",
              " 'ree': 878,\n",
              " 'canadian': 879,\n",
              " 'ditch': 880,\n",
              " 'bonding': 881,\n",
              " 'dude': 882,\n",
              " 'engine': 883,\n",
              " 'remains': 884,\n",
              " 'pretended': 885,\n",
              " \"d'ya\": 886,\n",
              " 'sharks': 887,\n",
              " 'incredibly': 888,\n",
              " '.,do': 889,\n",
              " 'armadillo': 890,\n",
              " 'spoil': 891,\n",
              " 'card': 892,\n",
              " 'reconfiguration': 893,\n",
              " 'background': 894,\n",
              " 'mail': 895,\n",
              " 'probably': 896,\n",
              " 'genie': 897,\n",
              " 'vagina': 898,\n",
              " 'sailing': 899,\n",
              " 'i': 900,\n",
              " 'mugs': 901,\n",
              " 'teeny': 902,\n",
              " 'sonogram': 903,\n",
              " 'baddest': 904,\n",
              " 'robots': 905,\n",
              " 'mets': 906,\n",
              " 'benji': 907,\n",
              " 'pretzel': 908,\n",
              " 'crab': 909,\n",
              " 'celebration': 910,\n",
              " 'missing': 911,\n",
              " 'regional': 912,\n",
              " 'goodacre': 913,\n",
              " 'favourite': 914,\n",
              " 'ride': 915,\n",
              " 'tie': 916,\n",
              " 'bachelor': 917,\n",
              " 'yyou': 918,\n",
              " 'snap': 919,\n",
              " \"chef's\": 920,\n",
              " 'saving': 921,\n",
              " 'crystal': 922,\n",
              " 'reality': 923,\n",
              " 'seed': 924,\n",
              " 'certificate': 925,\n",
              " \"barney's\": 926,\n",
              " 'proudly': 927,\n",
              " 'coast': 928,\n",
              " 'huge': 929,\n",
              " 'shining': 930,\n",
              " 'violent': 931,\n",
              " 'plane': 932,\n",
              " 'countdown': 933,\n",
              " 'kurt': 934,\n",
              " 'talk': 935,\n",
              " \"daddy's\": 936,\n",
              " 'powerful': 937,\n",
              " 'mitts': 938,\n",
              " 'somebody': 939,\n",
              " 'tarragon': 940,\n",
              " 'six': 941,\n",
              " 'pearl': 942,\n",
              " 'tribbiani': 943,\n",
              " 'flock': 944,\n",
              " 'peace': 945,\n",
              " 'thumb': 946,\n",
              " 'hoop': 947,\n",
              " '.y': 948,\n",
              " 'ass': 949,\n",
              " 'ju': 950,\n",
              " 'cigarettes': 951,\n",
              " 'due': 952,\n",
              " 'bitch': 953,\n",
              " 'lonely': 954,\n",
              " 'laughter': 955,\n",
              " 'misses': 956,\n",
              " 'dial': 957,\n",
              " 'emotionally': 958,\n",
              " 'reject': 959,\n",
              " 'saucy': 960,\n",
              " 'big': 961,\n",
              " 'winter': 962,\n",
              " 'las': 963,\n",
              " 'lurker': 964,\n",
              " 'body': 965,\n",
              " 'tweet': 966,\n",
              " 'sailor': 967,\n",
              " 'elevator': 968,\n",
              " '3b': 969,\n",
              " 'eloping': 970,\n",
              " 'ewwww': 971,\n",
              " \".'\": 972,\n",
              " 'flyers': 973,\n",
              " 'wha': 974,\n",
              " \"'cha\": 975,\n",
              " 'fascinated': 976,\n",
              " 'connected': 977,\n",
              " 'diet': 978,\n",
              " 'dollars': 979,\n",
              " 'react': 980,\n",
              " 'freezer': 981,\n",
              " 'bald': 982,\n",
              " \"'so\": 983,\n",
              " 'lifted': 984,\n",
              " 'rope': 985,\n",
              " 'lecture': 986,\n",
              " 'confidence': 987,\n",
              " '.,but': 988,\n",
              " 'buffayhannigan': 989,\n",
              " 'rog': 990,\n",
              " 'awake': 991,\n",
              " 'electricity': 992,\n",
              " 'sewer': 993,\n",
              " 'insist': 994,\n",
              " 'a': 995,\n",
              " 'grandmother': 996,\n",
              " 'based': 997,\n",
              " 'personal': 998,\n",
              " 'referring': 999,\n",
              " ...}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wfrp3uwVJBr",
        "colab_type": "code",
        "outputId": "4f4069dd-f2d4-4f60-a284-fdcd26a8e7e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "chat_pair[1]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sounds like a date to me .,alright , so i am back in high school , i am standing in the middle of the cafeteria , and i realize i am totally naked .',\n",
              " 'alright , so i am back in high school , i am standing in the middle of the cafeteria , and i realize i am totally naked .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho-4Q7g4VJB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# batch_size = 5\n",
        "# 'tied'\n",
        "sentence = 'i am tied of working here. do you have any plans for today'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb54hpxzVJB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "abc = [word2index[word] for word in sentence.split() if word in word2index.keys()]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7JNQXCUVJB-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def find_max_length(sentences):\n",
        "    whole_sentence = []\n",
        "    for sentence in sentences:\n",
        "        sentence_remove = []\n",
        "        for word in sentence.split():\n",
        "            if word in word2index.keys():\n",
        "                sentence_remove.append(word)\n",
        "        whole_sentence.append(' '.join(sentence_remove))\n",
        "    return whole_sentence, max([len(sent) for sent in whole_sentence])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1QjtGO5VJCD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "whole_sentence, max_length = find_max_length(list_of_word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Qnk6NLAVJCI",
        "colab_type": "code",
        "outputId": "6f508201-5ef3-4afc-e142-d3206ee1661b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "length = []\n",
        "for sent in whole_sentence:\n",
        "    try:\n",
        "        len_ = len(sent.split())\n",
        "        length.append(len_)\n",
        "        if len_ == 259:\n",
        "            print(sent)    \n",
        "    except:\n",
        "        print(sent)\n",
        "max_length = max(length)\n",
        "max_length"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "okay it is time for the toast ! umm now now , i know that ross usually gives the toast , but this year i am going to do it .,no , no it is going to be great . really ! mom , dad , when i got married , one of the things that made me sure i could do it was the amazing example the two of you set for me . for that and so many other things i want to say thank you . i know i probably do not say it enough , but i love you . when i look around this room , i am by the thought of those who could not be here with us . nana , my beloved grandmother who would so want to be here , but she cannot because she is dead . as is our dog chichi . i mean look how cute she is . . was . do me a favor and pass this to my parents . remember she is dead . okay , her and nana , gone . wow ! hey does anybody remember when had to say goodbye to her children in terms of ? did not see that ? no movie fans ? ! you want to hear something sad ? the other day i was watching 60 minutes these in , who have been so , they were of love . you people are made of stone ! here's to mom and dad ! whatever !\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "259"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEiK_UvrYleK",
        "colab_type": "code",
        "outputId": "5ea50618-192f-4438-98f4-94276d3085d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "EOS_token = -1\n",
        "def make_equal_length(total_tensor):\n",
        "    max_length = max([len(sent) for sent in total_tensor])\n",
        "    for sent in total_tensor:\n",
        "        if len(sent) < max_length:\n",
        "            \n",
        "    \n",
        "def input_process(input_data, word2index):\n",
        "    total_tensor = []\n",
        "    max_input_length = max([len(sent.split()) for sent in input_data])\n",
        "    for input_sent in input_data:\n",
        "        tensor = [0] * max_input_length\n",
        "        tensor = [tensor.insert() for index,word in enumerate(input_sent)] + [EOS_token]\n",
        "        total_tensor.append(tensor)\n",
        "    format_input = make_equal_length(total_tensor)\n",
        "    \n",
        "\n",
        "def prepare_batch(batches_group, word2index, index2word):\n",
        "    input_data = [batch[0] for batch in batches_group]\n",
        "    output_data = [batch[1] for batch in bathces]\n",
        "    length = max( len(sent.split()) for sent in input_data)\n",
        "    input_processed = input_process(input_data, word2index)\n",
        "\n",
        "\n",
        "small_batch = 5\n",
        "batches_group = [random.choice(chat_pair) for _num in range(small_batch)]"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-29-e99b46ee5f15>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    max_length = max(sum([len(sent) for sent in batches_group,],[]))\u001b[0m\n\u001b[0m                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKGoy75UY2LZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d4f06554-fc4b-4062-d7c0-c01ce066458a"
      },
      "source": [
        "import random\n",
        "import torch\n",
        "import itertools\n",
        "\n",
        "EOS_token = -1\n",
        "PAD_token = 0\n",
        "def indexesFromSentence(word2index, sentence):\n",
        "#     print(sentence)\n",
        "    listdata = []\n",
        "    for word in sentence.split():\n",
        "        try:\n",
        "            listdata.append(word2index[word])\n",
        "        except:\n",
        "            pass\n",
        "    return_list = listdata + [EOS_token]\n",
        "    return return_list\n",
        "\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "# Returns padded input sequence tensor and lengths\n",
        "def inputVar(l, word2index):\n",
        "    indexes_batch = [indexesFromSentence(word2index, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "# Returns padded target sequence tensor, padding mask, and max target length\n",
        "def outputVar(l, word2index):\n",
        "    indexes_batch = [indexesFromSentence(word2index, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.ByteTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "# Returns all items for a given batch of pairs\n",
        "def batch2TrainData(word2index, pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, word2index)\n",
        "    output, mask, max_target_len = outputVar(output_batch, word2index)\n",
        "    return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "\n",
        "# Example for validation\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(word2index, [random.choice(chat_pair) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "print(\"input_variable:\", input_variable)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)\n"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_variable: tensor([[5501,  900,  900, 4168, 2185],\n",
            "        [ 817, 2146, 2800, 3095, 1485],\n",
            "        [ 322, 5595, 4999,  198,   -1],\n",
            "        [1604, 5066, 5269,  676,    0],\n",
            "        [6054, 1138, 2565, 1842,    0],\n",
            "        [1485, 3401, 1485, 5066,    0],\n",
            "        [4281, 6149, 5605, 5426,    0],\n",
            "        [ 817, 5269, 5066, 1219,    0],\n",
            "        [3379, 5066, 4360, 1485,    0],\n",
            "        [1485,  900, 1485,   -1,    0],\n",
            "        [3497,  298,   -1,    0,    0],\n",
            "        [6500, 3080,    0,    0,    0],\n",
            "        [2755, 1792,    0,    0,    0],\n",
            "        [4048,  574,    0,    0,    0],\n",
            "        [2205, 1485,    0,    0,    0],\n",
            "        [1485,   -1,    0,    0,    0],\n",
            "        [  43,    0,    0,    0,    0],\n",
            "        [ 198,    0,    0,    0,    0],\n",
            "        [6409,    0,    0,    0,    0],\n",
            "        [4281,    0,    0,    0,    0],\n",
            "        [4281,    0,    0,    0,    0],\n",
            "        [5710,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [4281,    0,    0,    0,    0],\n",
            "        [1138,    0,    0,    0,    0],\n",
            "        [3401,    0,    0,    0,    0],\n",
            "        [5220,    0,    0,    0,    0],\n",
            "        [2337,    0,    0,    0,    0],\n",
            "        [ 900,    0,    0,    0,    0],\n",
            "        [6374,    0,    0,    0,    0],\n",
            "        [3787,    0,    0,    0,    0],\n",
            "        [5501,    0,    0,    0,    0],\n",
            "        [5539,    0,    0,    0,    0],\n",
            "        [4357,    0,    0,    0,    0],\n",
            "        [1731,    0,    0,    0,    0],\n",
            "        [1736,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [ 900,    0,    0,    0,    0],\n",
            "        [3450,    0,    0,    0,    0],\n",
            "        [3497,    0,    0,    0,    0],\n",
            "        [ 267,    0,    0,    0,    0],\n",
            "        [ 384,    0,    0,    0,    0],\n",
            "        [5817,    0,    0,    0,    0],\n",
            "        [4795,    0,    0,    0,    0],\n",
            "        [2512,    0,    0,    0,    0],\n",
            "        [3787,    0,    0,    0,    0],\n",
            "        [6637,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [4281,    0,    0,    0,    0],\n",
            "        [ 900,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [5269,    0,    0,    0,    0],\n",
            "        [2182,    0,    0,    0,    0],\n",
            "        [5972,    0,    0,    0,    0],\n",
            "        [3095,    0,    0,    0,    0],\n",
            "        [ 900,    0,    0,    0,    0],\n",
            "        [4340,    0,    0,    0,    0],\n",
            "        [2133,    0,    0,    0,    0],\n",
            "        [2337,    0,    0,    0,    0],\n",
            "        [ 995,    0,    0,    0,    0],\n",
            "        [5699,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [ 900,    0,    0,    0,    0],\n",
            "        [4270,    0,    0,    0,    0],\n",
            "        [3195,    0,    0,    0,    0],\n",
            "        [ 995,    0,    0,    0,    0],\n",
            "        [5465,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [4281,    0,    0,    0,    0],\n",
            "        [1518,    0,    0,    0,    0],\n",
            "        [3095,    0,    0,    0,    0],\n",
            "        [2876,    0,    0,    0,    0],\n",
            "        [5399,    0,    0,    0,    0],\n",
            "        [3940,    0,    0,    0,    0],\n",
            "        [5066,    0,    0,    0,    0],\n",
            "        [ 900,    0,    0,    0,    0],\n",
            "        [2800,    0,    0,    0,    0],\n",
            "        [1532,    0,    0,    0,    0],\n",
            "        [6088,    0,    0,    0,    0],\n",
            "        [4267,    0,    0,    0,    0],\n",
            "        [2923,    0,    0,    0,    0],\n",
            "        [ 995,    0,    0,    0,    0],\n",
            "        [ 676,    0,    0,    0,    0],\n",
            "        [ 633,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [5501,    0,    0,    0,    0],\n",
            "        [3497,    0,    0,    0,    0],\n",
            "        [ 267,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [  43,    0,    0,    0,    0],\n",
            "        [ 198,    0,    0,    0,    0],\n",
            "        [1690,    0,    0,    0,    0],\n",
            "        [5568,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [  43,    0,    0,    0,    0],\n",
            "        [ 198,    0,    0,    0,    0],\n",
            "        [ 995,    0,    0,    0,    0],\n",
            "        [5668,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [3880,    0,    0,    0,    0],\n",
            "        [ 995,    0,    0,    0,    0],\n",
            "        [5699,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [1485,    0,    0,    0,    0],\n",
            "        [1970,    0,    0,    0,    0],\n",
            "        [5458,    0,    0,    0,    0],\n",
            "        [1792,    0,    0,    0,    0],\n",
            "        [3195,    0,    0,    0,    0],\n",
            "        [3095,    0,    0,    0,    0],\n",
            "        [5699,    0,    0,    0,    0],\n",
            "        [ 118,    0,    0,    0,    0],\n",
            "        [  -1,    0,    0,    0,    0]])\n",
            "lengths: tensor([123,  16,  11,  10,   3])\n",
            "target_variable: tensor([[ 817, 6445, 2860, 5153, 3477],\n",
            "        [1792, 5066, 1485, 5066, 1485],\n",
            "        [3195, 1138, 5605, 5426,   -1],\n",
            "        [3095, 3401, 5066, 1219,    0],\n",
            "        [5699, 6149, 4360, 1485,    0],\n",
            "        [ 118, 5269, 1485,   -1,    0],\n",
            "        [  -1, 5066,   -1,    0,    0],\n",
            "        [   0,  900,    0,    0,    0],\n",
            "        [   0,  298,    0,    0,    0],\n",
            "        [   0, 3080,    0,    0,    0],\n",
            "        [   0, 1792,    0,    0,    0],\n",
            "        [   0,  574,    0,    0,    0],\n",
            "        [   0, 1485,    0,    0,    0],\n",
            "        [   0,   -1,    0,    0,    0]])\n",
            "mask: tensor([[1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 1],\n",
            "        [1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 1, 0],\n",
            "        [1, 1, 1, 0, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0],\n",
            "        [0, 1, 0, 0, 0]], dtype=torch.uint8)\n",
            "max_target_len: 14\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP37c9qWd7_j",
        "colab_type": "code",
        "outputId": "faff0bf2-8abe-4bea-e6ad-c02ac6e61bb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pprint(index2word[5000])"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'studying'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlzHGZ30-zlF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from pytorch\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "\n",
        "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
        "        #   because our input size is a word embedding with number of features == hidden_size\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Convert word indexes to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # Forward pass through GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # Sum bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        # Return output and final hidden state\n",
        "        return outputs, hidden\n",
        "    \n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights (energies) based on the given method\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        # Get embedding of current input word\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # Return output and final hidden state\n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK3YsrQf-90h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JuC4aq4NtPh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_LENGTH = 15\n",
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
        "\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable\n",
        "    lengths = lengths\n",
        "    target_variable = target_variable\n",
        "    mask = mask\n",
        "\n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # Perform backpropatation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztn1SfAaN0U7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
        "\n",
        "    # Load batches for each iteration\n",
        "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "\n",
        "    # Initializations\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        # Extract fields from batch\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7Q73flEOBGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        # Forward input through encoder model\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        # Initialize decoder input with SOS_token\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "        # Initialize tensors to append decoded words to\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        # Iteratively decode one word token at a time\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Obtain most likely word token and its softmax score\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            # Record token and score\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            # Prepare current token to be next decoder input (add a dimension)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        # Return collections of word tokens and scores\n",
        "        return all_tokens, all_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKp0NwPuOFVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    ### Format input sentence as a batch\n",
        "    # words -> indexes\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Transpose dimensions of batch to match models' expectations\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # Use appropriate device\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    # Decode sentence with searcher\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    # indexes -> words\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            # Get input sentence\n",
        "            input_sentence = input('> ')\n",
        "            # Check if it is quit case\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            # Normalize sentence\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            # Evaluate sentence\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            # Format and print response sentence\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRUyMU-GOLMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e1449e92-57a1-4b18-ff3c-3a4e05c79ab0"
      },
      "source": [
        "# Configure models\n",
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'\n",
        "#attn_model = 'general'\n",
        "#attn_model = 'concat'\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename = None\n",
        "checkpoint_iter = 4000\n",
        "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
        "\n",
        "\n",
        "# Load model if a loadFilename is provided\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(len(words), hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, len(words), decoder_n_layers, dropout)\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder\n",
        "decoder = decoder\n",
        "print('Models built and ready to go!')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "go67AZTPP9vb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "67c941d9-3b07-4134-beb2-21f43b742858"
      },
      "source": [
        "import os\n",
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 4000\n",
        "print_every = 1\n",
        "save_every = 500\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# Run training iterations\n",
        "corpus_name = 'data'\n",
        "loadFilename = False\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, word2index, chat_pair, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building optimizers ...\n",
            "Starting Training!\n",
            "Initializing ...\n",
            "Training...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-d3218dedf0f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m trainIters(model_name, word2index, chat_pair, encoder, decoder, encoder_optimizer, decoder_optimizer,\n\u001b[1;32m     28\u001b[0m            \u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_n_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_n_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_iteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m            print_every, save_every, clip, corpus_name, loadFilename)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-49-884b1c7d04c7>\u001b[0m in \u001b[0;36mtrainIters\u001b[0;34m(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# Run a training iteration with batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n\u001b[0;32m---> 23\u001b[0;31m                      decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mprint_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-4846a9cef8fb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# Forward pass through encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Create initial decoder input (start with SOS tokens for each sentence)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-3b63cbbc7889>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, input_lengths, hidden)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Convert word indexes to embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# Pack padded batch of sequences for RNN module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m         return F.embedding(\n\u001b[1;32m    116\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1504\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1506\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: index out of range at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:193"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKei5JodOVg2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set dropout layers to eval mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(encoder, decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nusX58rnOWnV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d363a336-543c-4ebe-a6bb-aa64e08c481e"
      },
      "source": [
        "print(len(index2word) + 1)\n",
        "print(len(word2index))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6757\n",
            "6756\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhJYWrWoTSdC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}